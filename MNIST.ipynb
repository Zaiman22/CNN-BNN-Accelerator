{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR With BNN\n",
    "\n",
    "one of the thing you can make as a newbie with NN is a MNIST clasification. This file is praticly the same as the [Introduction to BNNs with Larq](https://docs.larq.dev/larq/tutorials/mnist/) with extra comment and gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environement\n",
    "\n",
    "The LARQ library only compatitble with a certain version of tensorflow, hence also the cuda, so do keep in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 01:00:12.642971: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-06 01:00:12.934972: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 01:00:14.254779: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/zaim22/anaconda3/envs/larq-env/lib:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib:/usr/local/cuda-12.8/lib64:\n",
      "2025-04-06 01:00:14.254948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/zaim22/anaconda3/envs/larq-env/lib:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib:/usr/local/cuda-12.8/lib64:\n",
      "2025-04-06 01:00:14.254961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Larq version: 0.13.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "import larq as lq\n",
    "print(\"Larq version:\", lq.__version__)\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available:\", tf.test.is_built_with_cuda())\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All quantized layers except the first will use the same options\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# In the first layer we only quantize the weights and not the input\n",
    "model.add(lq.layers.QuantConv2D(32, (3, 3),\n",
    "                                kernel_quantizer=\"ste_sign\",\n",
    "                                kernel_constraint=\"weight_clip\",\n",
    "                                use_bias=False,\n",
    "                                input_shape=(28, 28, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(tf.keras.layers.Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+sequential stats------------------------------------------------------------------------------------------+\n",
      "| Layer                  Input prec.           Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs  32-bit MACs |\n",
      "|                              (bit)                        x 1       x 1    (kB)                          |\n",
      "+----------------------------------------------------------------------------------------------------------+\n",
      "| quant_conv2d                     -  (-1, 26, 26, 32)      288         0    0.04           0       194688 |\n",
      "| max_pooling2d                    -  (-1, 13, 13, 32)        0         0       0           0            0 |\n",
      "| batch_normalization              -  (-1, 13, 13, 32)        0        64    0.25           0            0 |\n",
      "| quant_conv2d_1                   1  (-1, 11, 11, 64)    18432         0    2.25     2230272            0 |\n",
      "| max_pooling2d_1                  -    (-1, 5, 5, 64)        0         0       0           0            0 |\n",
      "| batch_normalization_1            -    (-1, 5, 5, 64)        0       128    0.50           0            0 |\n",
      "| quant_conv2d_2                   1    (-1, 3, 3, 64)    36864         0    4.50      331776            0 |\n",
      "| batch_normalization_2            -    (-1, 3, 3, 64)        0       128    0.50           0            0 |\n",
      "| flatten                          -         (-1, 576)        0         0       0           0            0 |\n",
      "| quant_dense                      1          (-1, 64)    36864         0    4.50       36864            0 |\n",
      "| batch_normalization_3            -          (-1, 64)        0       128    0.50           0            0 |\n",
      "| quant_dense_1                    1          (-1, 10)      640         0    0.08         640            0 |\n",
      "| batch_normalization_4            -          (-1, 10)        0        20    0.08           0            0 |\n",
      "| activation                       -          (-1, 10)        0         0       0           ?            ? |\n",
      "+----------------------------------------------------------------------------------------------------------+\n",
      "| Total                                                   93088       468   13.19     2599552       194688 |\n",
      "+----------------------------------------------------------------------------------------------------------+\n",
      "+sequential summary----------------------------+\n",
      "| Total params                      93.6 k     |\n",
      "| Trainable params                  93.1 k     |\n",
      "| Non-trainable params              468        |\n",
      "| Model size                        13.19 KiB  |\n",
      "| Model size (8-bit FP weights)     11.82 KiB  |\n",
      "| Float-32 Equivalent               365.45 KiB |\n",
      "| Compression Ratio of Memory       0.04       |\n",
      "| Number of MACs                    2.79 M     |\n",
      "| Ratio of MACs that are binarized  0.9303     |\n",
      "+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "lq.models.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 7s 6ms/step - loss: 0.4123 - accuracy: 0.9835\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 0.4120 - accuracy: 0.9837\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 0.4089 - accuracy: 0.9842\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 0.4109 - accuracy: 0.9830\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 0.4113 - accuracy: 0.9830\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.4093 - accuracy: 0.9836\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.4090 - accuracy: 0.9839\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.4101 - accuracy: 0.9830\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.4087 - accuracy: 0.9839\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 0.4116 - accuracy: 0.9836\n",
      "Total Training Time: 1.04 min\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4380 - accuracy: 0.9791\n",
      "Test Accuracy: 0.9790999889373779\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):  # Use '/GPU:0' for the first GPU or '/CPU:0' for CPU\n",
    "    # Compile the model (this will also use the GPU if available)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_images, train_labels, batch_size=64, epochs=10)\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print('Total Training Time: %.2f min' % ((end_time - start_time) / 60))\n",
    "\n",
    "# Evaluate the model (this will also use the GPU if available)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a random test image\n",
    "index = np.random.randint(0, len(test_images))\n",
    "image = test_images[index]\n",
    "true_label = test_labels[index]\n",
    "\n",
    "# Predict the label\n",
    "predictions = model.predict(image[np.newaxis, ...])\n",
    "predicted_label = np.argmax(predictions)\n",
    "\n",
    "# Display the image and prediction\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"True Label: {true_label}, Predicted: {predicted_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "larq-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
